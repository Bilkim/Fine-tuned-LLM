{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "huggingface-hub 0.19.4 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n",
      "multiprocess 0.70.15 requires dill>=0.3.7, but you'll have dill 0.3.5.1 which is incompatible.\n",
      "datasets 2.15.0 requires tqdm>=4.62.1, but you'll have tqdm 4.50.2 which is incompatible.\n",
      "evaluate 0.4.1 requires tqdm>=4.62.1, but you'll have tqdm 4.50.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\bill\\anaconda3\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp38-none-win_amd64.whl (2.2 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.0-cp38-none-win_amd64.whl (277 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\bill\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Collecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\bill\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.9.0-cp38-cp38-win_amd64.whl (366 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.1-cp38-cp38-win_amd64.whl (24.6 MB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\bill\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bill\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0; python_version < \"3.11\" in c:\\users\\bill\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp38-cp38-win_amd64.whl (61 kB)\n",
      "Installing collected packages: yarl, aiohttp, fsspec, huggingface-hub, tokenizers, safetensors, transformers, pyarrow-hotfix, xxhash, pyarrow, multiprocess, datasets, responses, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.8.3\n",
      "    Uninstalling fsspec-0.8.3:\n",
      "      Successfully uninstalled fsspec-0.8.3\n",
      "Successfully installed aiohttp-3.9.0 datasets-2.15.0 evaluate-0.4.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 pyarrow-14.0.1 pyarrow-hotfix-0.5 responses-0.18.0 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2 xxhash-3.4.1 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yelp Review data\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small training and evaluation sets\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained BERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Initializing the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=lambda p: {\"accuracy\": load(\"accuracy\").compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model after training\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing with the fine-tuned language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"Let this review be your reason to visit. We were celebrating my birthday this evening, and I was so happy to share my favorite meals that remind me of home with my American friends. Everyone at the table loved the food. Excellent and attentive service complemented incredible food. I am so grateful for every person who works at Nene’s.\",\n",
    "    \"First time having Ukrainian food! It was delicious, but we waited about 1h20min for our entrees. I think that discourages me a bit to come back soon, despite the great food and atmosphere. I would still recommend this place.\",\n",
    "    \"In the evening the establishment was full, but we were served quickly enough, the waiter was very nice and quite attentive). Of all the dishes I liked the kharcho soup, very tasty). Pancakes with cottage cheese are tasty, but a little undercooked). Khachapuri is not tasty at all, as if it came from frozen. The dough is hard, there is not enough cheese. Draniki are half raw. Pkhali are not similar to pkhali.\",\n",
    "    \"Little bit disappointed. The food tastes good. Nice aromatic seasonings. Food processing time is heavily diversified Ajaruli Khachapuri was burned and cheese was hard. Might be it was warmed up second time in the oven. Pic is included. Khinkali was good.\",\n",
    "    \"Today we were served rotten fish on potato pancakes. The fish smelled terrible. Never in 9 years in the United States have I been served rotten fish. We were with a small child, I can’t even imagine what would have happened to the child if he had eaten rotten fish. This would be our third time at the restaurant. Service is always great, food is mediocre. . . but, guys, rotten fish, seriously? I wanted to support this restaurant because the idea is amazing, but after today I can’t.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trained model is accessed from the trainer\n",
    "model = trainer.model\n",
    "\n",
    "# Tokenize the reviews\n",
    "tokenized_reviews = tokenizer(reviews, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference\n",
    "model.eval() # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_reviews)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "# Convert predictions to list for easy viewing\n",
    "predicted_ratings = predictions.tolist()\n",
    "\n",
    "# Print the predicted ratings\n",
    "for review, rating in zip(reviews, predicted_ratings):\n",
    "    print(f\"Review: {review}\\nPredicted Rating: {rating}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Food101 dataset\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")\n",
    "food = food.train_test_split(test_size=0.2)\n",
    "\n",
    "# Creating label dictionaries\n",
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image processor for ViT\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "# Image transformation\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n",
    "\n",
    "# Preprocessing function\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "# Apply preprocessing\n",
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Load the pretrained ViT model\n",
    "model = ViTForImageClassification.from_pretrained(checkpoint, num_labels=len(labels), id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_vit\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: {\"accuracy\": load(\"accuracy\").compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model after training\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Inference with the fine-tuned vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation dataset\n",
    "ds = load_dataset(\"food101\", split=\"validation[:1000]\")\n",
    "\n",
    "# Loading model\n",
    "fine_tuned_model = trainer.model\n",
    "\n",
    "# Selected indices for inference\n",
    "indices = [0, 299, 599, 899]\n",
    "\n",
    "# Preprocess and perform inference\n",
    "for idx in indices:\n",
    "    # Original image\n",
    "    image = ds[idx][\"image\"]\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Preprocess the image\n",
    "    processed_image = _transforms(image.convert(\"RGB\")).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(processed_image)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class = id2label[str(prediction.item())]\n",
    "    print(f\"Image Index: {idx}, Predicted Class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
